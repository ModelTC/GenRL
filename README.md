<div align="center">

<table>
  <tr>
    <td>
      <img src="assets/logo.webp" alt="GenRL Logo" width="100">
    </td>
    <td style="padding-left: 12px; text-align: left;">
      <h1 style="margin-bottom: 4px;">GenRL</h1>
      <h3 style="margin-top: 0;">Reinforcement Learning Framework for Visual Generation</h3>
    </td>
  </tr>
</table>

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.6](https://img.shields.io/badge/pytorch-2.6-ee4c2c.svg)](https://pytorch.org/)
[![License: Apache-2.0](https://img.shields.io/badge/license-Apache--2.0-orange.svg)](LICENSE.txt)
<!-- [![arXiv](https://img.shields.io/badge/arXiv-xxxx.xxxxx-b31b1b.svg)](https://arxiv.org/abs/xxxx.xxxxx) -->

<!-- TODO: Add a teaser image / GIF here -->
<!-- <img src="assets/teaser.png" width="800"> -->

**GenRL** is a scalable, modular reinforcement learning framework for optimizing visual generation models â€” from images to videos â€” with plug-and-play reward functions, multi-GPU distributed training, and first-class support for diffusion & flow-based generators.

[ğŸš€ Getting Started](#-getting-started) Â· [ğŸ“– Algorithms](#-supported-algorithms) Â· [ğŸ“Š Performance](#-performance) Â· [ğŸ—ï¸ Architecture](#ï¸-architecture)

</div>

---

## âœ¨ Highlights

- ğŸ¯ **Unified RL for Visual Generation** â€” A single framework covering text-to-image (T2I), text-to-video (T2V), and image-to-video (I2V) generation
- ğŸ”„ **Multi-Paradigm Support** â€” Native support for both **Diffusion** and **Rectified Flow** generation paradigms via unified SDE formulation
- ğŸ§© **Modular Reward System** â€” Plug-and-play reward functions: aesthetic scores, text-alignment, motion quality, OCR accuracy, and custom user-defined rewards
- âš¡ **Scalable & Efficient** â€” Multi-node FSDP training with activation checkpointing, LoRA / full fine-tune, EMA, 8-bit Adam, and memory-efficient reward model offloading
- ğŸ›ï¸ **YAML-Driven Configuration** â€” Everything from model choice, reward weights, training schedule to FSDP sharding strategy is controlled via a single YAML config
- ğŸ”¬ **Reproducible by Design** â€” Deterministic seeding across sampling, training, and logging for bit-exact experiment reproduction

---

## ğŸ“– Supported Algorithms

<!-- TODO: Add / update algorithm entries as they are implemented -->

| Algorithm | Type | Status | Description |
|-----------|------|--------|-------------|
| **[FlowGRPO](https://arxiv.org/abs/2505.05470)** | Policy Gradient | âœ… Supported | Group Relative Policy Optimization â€” compute advantages per-group with optional per-prompt stat tracking |
| **[MixGRPO](https://arxiv.org/abs/2507.21802)** | Policy Gradient | âœ… Supported | SDE sampling and GRPO-guided optimization only within the window  |
| **[CPS](https://arxiv.org/abs/2509.05952)** | Policy Gradient | âœ… Supported | A novel sampling formulation that adheres to the Coefficient-Preserving property  |
| **[LongCat-Video](https://arxiv.org/abs/2510.22200)** | Policy Gradient | âœ… Supported |  **Strong performance with multi-reward RLHF** |
| **[DiffusionNFT](https://arxiv.org/abs/2509.16117)** | Reward-conditioned Fine-tuning | ğŸš§ Coming Soon | Online RL paradigm that optimizes diffusion models directly on the forward process via flow matching |
| **[ReFL](https://arxiv.org/abs/2304.05977)** | Differentiable Reward Optimization | ğŸš§ Coming Soon | A direct tuning algorithm to optimize diffusion models against a scorer |
| **[DiffusionDPO](https://arxiv.org/abs/2311.12908)** | DPO | ğŸš§ Coming Soon | Direct Preference Optimization (DPO), a simpler alternative to RLHF which directly optimizes a policy under a classification objective. |

> ğŸ’¡ *GenRL is designed to be algorithm-agnostic. Adding a new RL algorithm only requires implementing a new trainer â€” everything else (rewards, data, logging) is reusable. For GRPO-based algorithms, most implementations only need to modify a small amount of code in the trainer.*

---

## ğŸ¤– Supported Models

<!-- TODO: Add / update model entries -->

| Model | Modality | Parameters | Status |
|-------|----------|------------|--------|
| [Wan2.1-T2V](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B-Diffusers) | Text â†’ Video | 1.3B | âœ… Supported |
| [Wan2.1-T2V](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B-Diffusers) | Text â†’ Video | 14B | âœ… Supported |
| [Wan2.2-T2V](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers) | Text â†’ Video | 14B | ğŸš§ Coming Soon |
| [Wan2.2-I2V](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers) | Image â†’ Video | 14B | ğŸš§ Coming Soon |
| [HunyuanImage-3.0-Instruct](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct) | Image â†’ Image | 80B | ğŸš§ Coming Soon |

---

## ğŸ Supported Reward Functions

| Reward | Domain | Source | Description |
|--------|--------|--------|-------------|
| `video_ocr` | ğŸ“ Text | Built-in | OCR accuracy reward â€” measures text rendering quality via PaddleOCR |
| `hpsv3_general` | ğŸ–¼ï¸ Aesthetics | [HPSv3](https://github.com/tgxs002/HPSv3) | Human Preference Score v3 â€” general aesthetic quality |
| `hpsv3_percentile` | ğŸ–¼ï¸ Aesthetics | [HPSv3](https://github.com/tgxs002/HPSv3) | HPSv3 percentile-based reward normalization |
| `videoalign_mq` | ğŸ¬ Motion | [VideoAlign](https://github.com/KwaiVGI/VideoAlign) | Video motion quality assessment |
| `videoalign_ta` | ğŸ¬ Alignment | [VideoAlign](https://github.com/KwaiVGI/VideoAlign) | Video text-alignment score |
| **Custom** | ğŸ”§ Any | User-defined | Bring your own reward via `reward_module` config |

> ğŸ”— Multiple rewards can be **composed with configurable weights** â€” GenRL supports both *reward-weighted* and *advantage-weighted* composition modes.

---

## ğŸ“Š Performance

<!-- TODO: Fill in actual numbers from your experiments -->

### ğŸ¬ Text-to-Video (Wan2.1-T2V 1.3B)

| Method | HPSv3 â†‘ | VideoAlign-MQ â†‘ | VideoAlign-TA â†‘ | Training Cost |
|--------|---------|-----------------|-----------------|---------------|
| Baseline (pretrained) | â€” | â€” | â€” | â€” |
| GenRL-GRPO (LoRA) | â€” | â€” | â€” | â€” |
| GenRL-GRPO (Full FT) | â€” | â€” | â€” | â€” |

### ğŸ“ Video OCR

| Method | OCR Accuracy â†‘ | Levenshtein Score â†‘ | Training Cost |
|--------|---------------|---------------------|---------------|
| Baseline (pretrained) | â€” | â€” | â€” |
| GenRL-GRPO (LoRA) | â€” | â€” | â€” |

<!-- ### ğŸ–¼ï¸ Text-to-Image -->
<!-- TODO: Add T2I benchmarks when available -->

> ğŸ“ˆ *Performance tables will be updated with results from ongoing experiments. Stay tuned!*

---

## ğŸš€ Getting Started

### ğŸ“‹ Prerequisites

- Python 3.10+
- CUDA 12.x + PyTorch 2.6
- 8Ã— A100/H100 GPUs (recommended for video training)

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Initialize Submodules

```bash
git submodule update --init --recursive
```

### 3ï¸âƒ£ Setup Reward Model Checkpoints

<details>
<summary>ğŸ¬ VideoAlign (for <code>videoalign_mq</code> / <code>videoalign_ta</code> rewards)</summary>

```bash
cd genrl/reward/VideoAlign/checkpoints
git lfs install
git clone https://huggingface.co/KwaiVGI/VideoReward
mv VideoReward/* .
mv VideoReward/.* . 2>/dev/null || true
rm -rf VideoReward
cd ../../../..
```
</details>

<details>
<summary>ğŸ“ PaddleOCR (for <code>video_ocr</code> reward)</summary>

```bash
python -c "from paddleocr import PaddleOCR; ocr = PaddleOCR(use_angle_cls=False, lang='en', use_gpu=False, show_log=False)"
```
</details>

<details>
<summary>ğŸ–¼ï¸ HPSv3 (for <code>hpsv3_general</code> / <code>hpsv3_percentile</code> rewards)</summary>

```bash
pip install flash-attn==2.7.4.post1 --no-build-isolation
```
</details>

### 4ï¸âƒ£ Launch Training

```bash
# Single node, 8 GPUs (LoRA + FSDP)
accelerate launch train.py --config config/default.yaml

# Multi-node (8 nodes Ã— 8 GPUs)
torchrun --nnodes=4 --nproc_per_node=8 \
  --rdzv_backend=c10d \
  --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
  train.py --config config/longcat.yaml
```

---

## ğŸ—ï¸ Architecture

```
GenRL/
â”œâ”€â”€ ğŸš€ train.py                        # Entry point
â”œâ”€â”€ ğŸ“ config/                          # YAML configs
â”‚   â”œâ”€â”€ default.yaml                    #   Default (OCR, FlowGRPO)
â”‚   â””â”€â”€ longcat.yaml                    #   Multi-reward, LongCat
â”œâ”€â”€ ğŸ“ genrl/
â”‚   â”œâ”€â”€ config.py                       # Config schema & loader
â”‚   â”œâ”€â”€ constants.py                    # Global constants
â”‚   â”œâ”€â”€ data.py                         # Dataset & dataloaders
â”‚   â”œâ”€â”€ rewards.py                      # Multi-reward composition
â”‚   â”œâ”€â”€ advantages.py                   # Advantage computation (GRPO)
â”‚   â”œâ”€â”€ stat_tracking.py                # Per-prompt stat tracking
â”‚   â”œâ”€â”€ ema.py                          # EMA wrapper
â”‚   â”œâ”€â”€ ğŸ“ trainer/
â”‚   â”‚   â”œâ”€â”€ base_trainer.py             #   Abstract base trainer
â”‚   â”‚   â”œâ”€â”€ wan_trainer.py              #   Wan model trainer
â”‚   â”‚   â”œâ”€â”€ sampling.py                 #   Sampling epoch logic
â”‚   â”‚   â”œâ”€â”€ evaluation.py               #   Eval & video logging
â”‚   â”‚   â”œâ”€â”€ diffusion.py                #   Log-prob computation
â”‚   â”‚   â””â”€â”€ embeddings.py               #   Text embedding utils
â”‚   â”œâ”€â”€ ğŸ“ reward/
â”‚   â”‚   â”œâ”€â”€ ocr.py                      #   OCR reward
â”‚   â”‚   â”œâ”€â”€ hpsv3.py                    #   HPSv3 reward
â”‚   â”‚   â”œâ”€â”€ videoalign.py               #   VideoAlign rewards
â”‚   â”‚   â”œâ”€â”€ ğŸ“ HPSv3/                   #   HPSv3 submodule
â”‚   â”‚   â””â”€â”€ ğŸ“ VideoAlign/              #   VideoAlign submodule
â”‚   â””â”€â”€ ğŸ“ diffusers_patch/
â”‚       â””â”€â”€ wan_pipeline_with_logprob.py  # SDE step with log-prob
â”œâ”€â”€ ğŸ“ datasets/                        # Prompt datasets
â””â”€â”€ ğŸ“ scripts/
    â””â”€â”€ launch.sh                       # Launch script
```

---

## âš™ï¸ Configuration

All training behavior is controlled by a single YAML file. Key sections:

| Section | What it controls |
|---------|-----------------|
| `reward_fn` | Reward functions & weights (e.g., `video_ocr: 1.0`, `hpsv3_general: 1.0`) |
| `sample` | Sampling: batch size, num steps, guidance scale, SDE type, noise level |
| `train` | Training: learning rate, clip range, advantage clipping, LoRA rank, EMA |
| `accelerate` | Distributed: FSDP, mixed precision, num GPUs/nodes |
| `paths` | Model path, dataset path, save directory, resume checkpoint |

<details>
<summary>ğŸ“„ Example config (<code>config/default.yaml</code>)</summary>

```yaml
run_name: my_experiment
seed: 42
num_epochs: 100000
height: 240
width: 416
frames: 33

reward_fn:
  video_ocr: 1.0

trainer: wan
use_lora: true

sample:
  batch_size: 8
  num_steps: 20
  guidance_scale: 4.5
  sde_type: flow_sde

train:
  learning_rate: 1.0e-4
  clip_range: 1.0e-3
  lora_r: 32
  ema: true

accelerate:
  distributed_type: FSDP
  mixed_precision: bf16
  num_processes: 8
```
</details>

---

## ğŸ“‚ Output Structure

```
logs/
â””â”€â”€ <experiment>/
    â””â”€â”€ <run_name>_<timestamp>/
        â”œâ”€â”€ ğŸ“ checkpoints/                    # Periodic checkpoints
        â”‚   â””â”€â”€ checkpoint-{step}/
        â”‚       â”œâ”€â”€ ema/                        # EMA states
        â”‚       â”œâ”€â”€ unwrapped_model/transformer/ # Model weights
        â”‚       â””â”€â”€ metadata.json               # Step & config metadata
        â”œâ”€â”€ ğŸ“ final_model/                    # Final trained model
        â”‚   â””â”€â”€ transformer/
        â”‚       â”œâ”€â”€ adapter_config.json         # LoRA config (if LoRA)
        â”‚       â””â”€â”€ adapter_model.safetensors   # LoRA weights (if LoRA)
        â”œâ”€â”€ ğŸ“ eval_videos/                    # Evaluation videos
        â””â”€â”€ ğŸ“ sample_videos/                  # Training sample videos
```

---

## ğŸ”‘ Key Features at a Glance

| Feature | Details |
|---------|---------|
| ğŸ¯ RL Algorithm | GRPO with per-prompt stat tracking & advantage clipping |
| ğŸ§¬ SDE Types | `flow_sde`, `flow_cps` â€” unified SDE formulation for rectified flow |
| ğŸªŸ Windowed Training | `sde_window_size` / `sde_window_range` for timestep sub-sampling |
| ğŸ“Š Reward Composition | Multi-reward weighted sum, advantage-weighted mode |
| ğŸ§® KL Regularization | Optional KL reward to constrain policy drift |
| ğŸšï¸ Guidance | Configurable classifier-free guidance for sampling & evaluation |
| ğŸ’¾ Checkpointing | Periodic + final model saves with FSDP sharded state dict |
| ğŸ“ˆ Logging | WandB integration with training curves, sample videos, eval videos |
| ğŸ” EMA | Exponential moving average with configurable decay & update interval |
| ğŸ§© LoRA | PEFT LoRA with configurable rank, alpha, and target modules |
| ğŸ”’ Reproducibility | Deterministic seeding with `SEED_EPOCH_STRIDE` for all stochastic ops |

---

<!-- ## ğŸ“ Citation -->

<!-- ```bibtex -->
<!-- @article{genrl2026, -->
<!--   title={GenRL: Reinforcement Learning Framework for Visual Generation}, -->
<!--   author={}, -->
<!--   year={2026} -->
<!-- } -->
<!-- ``` -->

## ğŸ“ TODO

- **Model support**
  - Extend support for more text-to-image / image-to-image backbones beyond the current Wan / Hunyuan family
- **Algorithmic extensions**
  - Integrate more **GRPO-family** variants and related online RL algorithms
  - Add DPO / OnlineDPO, SFT / OnlineSFT style objectives alongside GRPO-style training
- **Rollout & parallelism**
  - Integrate **LightX2V** inference framework for accelerated rollout
  - Multi-level parallel rollout (e.g., **SP**, **HSDP**) for better hardware utilization
  - **Asynchronous rollout** workers with decoupled sampling/training pipelines
  - Improved multi-node orchestration utilities and monitoring for large-scale runs

---

## ğŸ™ Acknowledgements

GenRL is built upon the excellent work of the open-source community. We would like to thank:

- **[Flow-GRPO](https://github.com/yifan123/flow_grpo)** â€” We reference their implementation for the GRPO-based algorithm and training framework.

---

## ğŸ“„ License

GenRL is licensed under the **Apache License 2.0**.  
See `LICENSE.txt` for the full license text.

---

<div align="center">

**If you find GenRL useful, please give us a â­!**

</div>
